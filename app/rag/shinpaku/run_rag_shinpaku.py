# çœŸæŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–AIãƒãƒ£ãƒƒãƒˆ
import json
import os
import time
from typing import List, Tuple
from dataclasses import dataclass

from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.schema.messages import HumanMessage
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½ç½®ã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’è¨ˆç®—ï¼ˆã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¾å­˜ã—ãªã„ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_ROOT = os.path.join(SCRIPT_DIR, "..", "..", "..")
ENV_PATH = os.path.join(BACKEND_ROOT, ".env.local")

load_dotenv(dotenv_path=ENV_PATH)

TOP_K = 5  # æœ€çµ‚çš„ãªæ¤œç´¢çµæœæ•°
BM25_K = 3  # BM25æ¤œç´¢ã®å–å¾—æ•°
VECTOR_K = 3  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å–å¾—æ•°

VECTORSTORE_PATH = os.path.join(BACKEND_ROOT, "data", "vectorstore_shinpaku")

# OpenAI APIæ–™é‡‘è¨­å®šï¼ˆ2024å¹´1æœˆç¾åœ¨ï¼‰
PRICING = {
    "gpt-4o": {"input": 0.00250, "output": 0.01000},  # per 1K tokens
    "gpt-4o-mini": {"input": 0.00015, "output": 0.00060},  # per 1K tokens
    "gpt-4.1-nano": {"input": 0.0001, "output": 0.00040},  # gpt-4o-miniã¨åŒç­‰ã¨ä»®å®š
    "text-embedding-ada-002": {"input": 0.00010, "output": 0.0}  # per 1K tokens
}

@dataclass
class TokenUsage:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    model: str
    input_tokens: int = 0
    output_tokens: int = 0
    cost: float = 0.0
    
    def add_usage(self, input_tokens: int, output_tokens: int = 0):
        """ä½¿ç”¨é‡ã‚’è¿½åŠ ã—ã€ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
        self.input_tokens += input_tokens
        self.output_tokens += output_tokens
        
        if self.model in PRICING:
            pricing = PRICING[self.model]
            self.cost = (self.input_tokens * pricing["input"] + 
                        self.output_tokens * pricing["output"]) / 1000
    
    def __str__(self):
        return f"{self.model}: {self.input_tokens}in + {self.output_tokens}out = ${self.cost:.6f}"

class TokenTracker:
    """APIä½¿ç”¨é‡ã‚’è¿½è·¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.session_usage = {}
        self.total_usage = {}
        self.start_time = time.time()
    
    def track_usage(self, model: str, input_tokens: int, output_tokens: int = 0):
        """ä½¿ç”¨é‡ã‚’è¨˜éŒ²"""
        if model not in self.session_usage:
            self.session_usage[model] = TokenUsage(model)
            self.total_usage[model] = TokenUsage(model)
        
        self.session_usage[model].add_usage(input_tokens, output_tokens)
        self.total_usage[model].add_usage(input_tokens, output_tokens)
    
    def get_session_summary(self) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.session_usage:
            return "ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: ãªã—"
        
        summary = ["ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½¿ç”¨é‡:"]
        total_cost = 0.0
        
        for model, usage in self.session_usage.items():
            summary.append(f"  {usage}")
            total_cost += usage.cost
        
        duration = time.time() - self.start_time
        summary.append(f"  åˆè¨ˆã‚³ã‚¹ãƒˆ: ${total_cost:.6f}")
        summary.append(f"  ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {duration:.1f}ç§’")
        
        return "\n".join(summary)
    
    def get_query_summary(self, model: str, input_tokens: int, output_tokens: int = 0) -> str:
        """ã‚¯ã‚¨ãƒªå˜ä½ã®ä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if model in PRICING:
            pricing = PRICING[model]
            cost = (input_tokens * pricing["input"] + output_tokens * pricing["output"]) / 1000
            return f"ğŸ’° {model}: {input_tokens}in + {output_tokens}out = ${cost:.6f}"
        return f"ğŸ’° {model}: {input_tokens}in + {output_tokens}out"


class RAGChatBotShinpaku:
    def __init__(self, show_content=False, track_tokens=True):
        print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
        )
        self.vectorstore = Chroma(
            embedding_function=self.embeddings, persist_directory=VECTORSTORE_PATH
        )
        self.documents = self.vectorstore.get()
        self.docs = self.documents["documents"]
        self.metadatas = self.documents["metadatas"]
        
        print("BM25ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’åˆæœŸåŒ–ä¸­...")
        self.keyword_retriever = BM25Retriever.from_texts(
            self.documents["documents"],
            metadatas=self.documents["metadatas"],
            preprocess_func=self.preprocess_func,
            k=BM25_K  # BM25æ¤œç´¢ã®çµæœæ•°ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        )
        self.vector_retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": VECTOR_K}  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®çµæœæ•°ã‚’æŒ‡å®š
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        self.system_message = (
            """ã‚ãªãŸã¯ç›†æ ½ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã«ä¸ãˆã‚‰ã‚ŒãŸç›†æ ½ã®å°‚é–€æ–‡æ›¸ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            
            ã€å›ç­”æ–¹é‡ã€‘
            ãƒ»ç›†æ ½ã®å°‚é–€ç”¨èªã‚„æŠ€æ³•ã«ã¤ã„ã¦æ­£ç¢ºã«èª¬æ˜ã—ã¦ãã ã•ã„
            ãƒ»å°‚é–€æ–‡æ›¸ã«åŸºã¥ãå›ç­”ã‚’å„ªå…ˆã—ã€ä¸æ­£ç¢ºãªéƒ¨åˆ†ãŒã‚ã‚Œã°æ–­å®šã¯é¿ã‘ã¦ãã ã•ã„
            ãƒ»æ¨æ¸¬ã«åŸºã¥ãå›ç­”ã‚’ã™ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„
            ãƒ»å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆã¯ã€å®‰å…¨æ€§ã«ã‚‚é…æ…®ã—ã¦ãã ã•ã„
            
            ã€æ³¨æ„ç‚¹ã€‘
            ãƒ»å°‚é–€æ–‡æ›¸ã®ãƒ‡ãƒ¼ã‚¿ã¯ç›†æ ½é›‘èªŒã‹ã‚‰èª­ã¿å–ã£ãŸã‚‚ã®ã§ã‚ã‚‹ãŸã‚ã€æ–‡ç« ã®æ§‹é€ ãŒä¸è‡ªç„¶ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚
            ãƒ»å‡ºåŠ›ã¯ã§ãã‚‹ã ã‘å¹³æ–‡ã§è¡Œã„ã€markdownå½¢å¼ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"""
        )
        
        self.model = "gpt-4.1-nano"
        self.llm = ChatOpenAI(model=self.model)
        
        # ä¼šè©±å±¥æ­´ã‚’ä¿æŒ
        self.chat_history = []
        self.show_content = show_content
        
        # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡æ©Ÿèƒ½
        self.track_tokens = track_tokens
        self.token_tracker = TokenTracker() if track_tokens else None
        
        print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        if track_tokens:
            print("ğŸ’° ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¿½è·¡: æœ‰åŠ¹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        self.display_database_stats()

    def display_database_stats(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if not self.metadatas:
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆæƒ…å ±")
        print("=" * 60)
        
        total_docs = len(self.metadatas)
        print(f"ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_docs}")
        
        # æ–‡çŒ®åã®çµ±è¨ˆ
        æ–‡çŒ®å_set = set()
        æ¨¹ç¨®_set = set()
        ç« _set = set()
        
        for metadata in self.metadatas:
            if metadata.get('æ–‡çŒ®å'):
                æ–‡çŒ®å_set.add(metadata['æ–‡çŒ®å'])
            if metadata.get('æ¨¹ç¨®') and metadata['æ¨¹ç¨®'].strip():
                æ¨¹ç¨®_set.add(metadata['æ¨¹ç¨®'])
            if metadata.get('ç« ') and metadata['ç« '].strip():
                ç« _set.add(metadata['ç« '])
        
        print(f"æ–‡çŒ®æ•°: {len(æ–‡çŒ®å_set)}")
        print(f"æ¨¹ç¨®æ•°: {len(æ¨¹ç¨®_set)}")
        print(f"ç« æ•°: {len(ç« _set)}")
        
        if æ¨¹ç¨®_set:
            print(f"\nä¸»ãªæ¨¹ç¨®: {', '.join(sorted(list(æ¨¹ç¨®_set))[:10])}")
        
        print("=" * 60)

    def main(self):
        print("=" * 60)
        print("çœŸæŸãƒ»ç›†æ ½å°‚ç”¨AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
        print("=" * 60)
        print("çœŸæŸã‚„ç›†æ ½ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit', 'exit', 'q' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã«ã¯ 'clear' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        if self.track_tokens:
            print("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ç¢ºèªã™ã‚‹ã«ã¯ 'tokens' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("-" * 60)
        print("å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚‚è¡¨ç¤ºã™ã‚‹: {}".format("ON" if self.show_content else "OFF"))
        if self.track_tokens:
            print("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¿½è·¡: ON")
        
        while True:
            try:
                user_input = input("\nã‚ãªãŸ: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    if self.track_tokens and self.token_tracker:
                        print("\n" + self.token_tracker.get_session_summary())
                    print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                elif user_input.lower() == 'clear':
                    self.chat_history = []
                    print("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                    continue
                elif user_input.lower() == 'tokens' and self.track_tokens:
                    if self.token_tracker:
                        print("\n" + self.token_tracker.get_session_summary())
                    else:
                        print("ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ãŒç„¡åŠ¹ã§ã™ã€‚")
                    continue
                elif not user_input:
                    print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    continue
                
                print("\nğŸ” æ¤œç´¢ä¸­...")
                response, metadata_list, referenced_docs = self.run_chat(user_input)
                print(f"\nAI: {response}")
                
                # å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¡¨ç¤º
                self.display_referenced_documents(metadata_list, referenced_docs, show_content=self.show_content)
                
                # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                self.chat_history.append({"role": "user", "content": user_input})
                self.chat_history.append({"role": "assistant", "content": response})
                
                # ä¼šè©±å±¥æ­´ãŒé•·ããªã‚Šã™ããŸå ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
                if len(self.chat_history) > 10:
                    self.chat_history = self.chat_history[-10:]
                    
            except KeyboardInterrupt:
                print("\n\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue

    def run_chat(self, question: str) -> Tuple[str, list, list]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦RAGã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            
        Returns:
            Tuple[str, list, list]: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        result, metadata_list, referenced_docs = self.generate_output(
            self.keyword_retriever, 
            self.vector_retriever, 
            question, 
            self.system_message
        )
        return result, metadata_list, referenced_docs

    def preprocess_func(self, text: str) -> List[str]:
        """BM25ç”¨ã®å‰å‡¦ç†é–¢æ•°"""
        i, j = 3, 5
        if len(text) < i:
            return [text]
        return self.generate_character_ngrams(text, i, j, True)

    def generate_character_ngrams(self, text, i, j, binary=False):
        """
        æ–‡å­—åˆ—ã‹ã‚‰æŒ‡å®šã—ãŸæ–‡å­—æ•°ã®n-gramã‚’ç”Ÿæˆ
        
        :param text: æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿
        :param i: n-gramã®æœ€å°æ–‡å­—æ•°
        :param j: n-gramã®æœ€å¤§æ–‡å­—æ•°
        :param binary: Trueã®å ´åˆã€é‡è¤‡ã‚’å‰Šé™¤
        :return: n-gramã®ãƒªã‚¹ãƒˆ
        """
        ngrams = []

        for n in range(i, j + 1):
            for k in range(len(text) - n + 1):
                ngram = text[k : k + n]
                ngrams.append(ngram)

        if binary:
            ngrams = list(set(ngrams))  # é‡è¤‡ã‚’å‰Šé™¤

        return ngrams

    def generate_output(
        self, keyword_retriever, vector_retriever, user_input, system_message
    ):
        """RAGæ¤œç´¢ã¨å›ç­”ç”Ÿæˆ"""
        question = self.regenerate_question(user_input)
        keyword_searched = keyword_retriever.invoke(question)
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã®embeddingä½¿ç”¨é‡ã‚’è¿½è·¡
        if self.track_tokens and self.token_tracker:
            # ã‚¯ã‚¨ãƒªã®embeddingä½œæˆã‚³ã‚¹ãƒˆ
            query_tokens = len(question) // 4  # å¤§ã¾ã‹ãªæ¨å®š
            self.token_tracker.track_usage("text-embedding-ada-002", query_tokens, 0)
            print(f"    {self.token_tracker.get_query_summary('text-embedding-ada-002', query_tokens, 0)}")
        
        vector_searched = vector_retriever.invoke(question)
        
        # é‡è¤‡é™¤å»ã¨TOP_Kå€‹ã¸ã®åˆ¶é™
        unique_docs = []
        seen_contents = set()
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’å„ªå…ˆã—ã¦è¿½åŠ 
        for doc in vector_searched:
            content_hash = hash(doc.page_content)
            if content_hash not in seen_contents and len(unique_docs) < TOP_K:
                unique_docs.append(doc)
                seen_contents.add(content_hash)
        
        # æ®‹ã‚Šã®æ ã‚’BM25æ¤œç´¢çµæœã§åŸ‹ã‚ã‚‹
        for doc in keyword_searched:
            content_hash = hash(doc.page_content)
            if content_hash not in seen_contents and len(unique_docs) < TOP_K:
                unique_docs.append(doc)
                seen_contents.add(content_hash)
        
        # æœ€çµ‚çš„ã«TOP_Kå€‹ã«åˆ¶é™
        final_docs = unique_docs[:TOP_K]
        
        texts_retrieved = []
        metadata_list = []
        referenced_docs = []
        
        for doc in final_docs:
            texts_retrieved.append(doc.page_content)
            metadata_list.append(doc.metadata)
            referenced_docs.append(doc)
        
        print(f"ğŸ” æ¤œç´¢çµæœ: ãƒ™ã‚¯ãƒˆãƒ«{len(vector_searched)}ä»¶ + BM25 {len(keyword_searched)}ä»¶ â†’ é‡è¤‡é™¤å»å¾Œ{len(final_docs)}ä»¶")
        
        result = self.chat_based_on_texts(texts_retrieved, question, system_message)
        return result, metadata_list, referenced_docs

    def regenerate_question(self, user_input):
        """
        ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ã¦è³ªå•ã‚’å†ç”Ÿæˆã™ã‚‹
        """
        if not self.chat_history:
            return user_input
            
        prompt = ChatPromptTemplate.from_template(
            """
            æ¬¡ã®ä¼šè©±å±¥æ­´ã¨ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®è³ªå•ã‚’å…ƒã«ã€ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®è³ªå•ã‚’ç‹¬ç«‹ã—ãŸè³ªå•ã¨ã—ã¦è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚
            ç›†æ ½ã«é–¢ã™ã‚‹å°‚é–€çš„ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
            ----------
            ä¼šè©±ã®å±¥æ­´: {chat_history}
            ----------
            ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®è³ªå•: {follow_up_question}
            ----------
            ç‹¬ç«‹ã—ãŸè³ªå•:
            """
        )
        chain = prompt | self.llm
        follow_up_question = user_input
        args = {"chat_history": self.chat_history, "follow_up_question": follow_up_question}
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡
        if self.track_tokens and self.token_tracker:
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¦‚ç®—ï¼ˆæ­£ç¢ºã«ã¯å–å¾—å›°é›£ãªã®ã§æ¨å®šï¼‰
            input_text = str(self.chat_history) + follow_up_question
            estimated_input_tokens = len(input_text) // 4  # å¤§ã¾ã‹ãªæ¨å®š
            
        ans = chain.invoke(args)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²ï¼ˆæ¨å®šå€¤ï¼‰
        if self.track_tokens and self.token_tracker:
            output_tokens = len(str(ans.content)) // 4  # å¤§ã¾ã‹ãªæ¨å®š
            self.token_tracker.track_usage(self.model, estimated_input_tokens, output_tokens)
            print(f"    {self.token_tracker.get_query_summary(self.model, estimated_input_tokens, output_tokens)}")
            
        return str(ans.content)

    def chat_based_on_texts(self, texts_retrieved, question, system_message):
        """æ¤œç´¢çµæœã‚’åŸºã«ã—ãŸå›ç­”ç”Ÿæˆ"""
        texts = "\n\n".join(texts_retrieved)
        
        # ä¼šè©±å±¥æ­´ã‚’æ–‡å­—åˆ—å½¢å¼ã«å¤‰æ›
        history_str = ""
        for msg in self.chat_history[-4:]:  # æœ€æ–°ã®4ã¤ã®ç™ºè¨€ã®ã¿ä½¿ç”¨
            history_str += f"{msg['role']}: {msg['content']}\n"
        
        prompt_text = f"""
            {system_message}
            ----------
            ä¼šè©±è¨˜éŒ²ã‚’å…ƒã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã®ã‚­ãƒ£ãƒƒãƒãƒœãƒ¼ãƒ«ã‚’æˆç«‹ã•ã›ã¦ãã ã•ã„ã€‚
            ç›†æ ½ã®å°‚é–€çŸ¥è­˜ã‚’æ­£ç¢ºã«ä¼ãˆã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            ----------
            ä¼šè©±è¨˜éŒ²: {history_str}
            ----------
            å°‚é–€æ–‡æ›¸ã‹ã‚‰ã®å‚è€ƒæƒ…å ±: {texts}
            ----------
            è³ªå•: {question}
            """

        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡
        if self.track_tokens and self.token_tracker:
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¦‚ç®—
            estimated_input_tokens = len(prompt_text) // 4  # å¤§ã¾ã‹ãªæ¨å®š
            
        response = self.llm.invoke(
            [HumanMessage(content=[{"type": "text", "text": prompt_text}])]
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
        if self.track_tokens and self.token_tracker:
            output_tokens = len(response.content) // 4  # å¤§ã¾ã‹ãªæ¨å®š
            self.token_tracker.track_usage(self.model, estimated_input_tokens, output_tokens)
            print(f"    {self.token_tracker.get_query_summary(self.model, estimated_input_tokens, output_tokens)}")
            
        return response.content

    def display_referenced_documents(self, metadata_list, referenced_docs, show_content=False):
        """
        å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æƒ…å ±ã‚’æ•´ç†ã—ã¦è¡¨ç¤ºã™ã‚‹ï¼ˆæ•´å‚™æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼‰
        Args:
            metadata_list (list): ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
            referenced_docs (list): å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            show_content (bool): ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚‚è¡¨ç¤ºã™ã‚‹ã‹
        """
        if not metadata_list:
            return
            
        doc_info = {}
        
        # metadataã¨docsã‚’å¯¾å¿œä»˜ã‘ã¦å‡¦ç†
        for i, (metadata, doc) in enumerate(zip(metadata_list, referenced_docs)):
            æ–‡çŒ®å = metadata.get('æ–‡çŒ®å', 'ä¸æ˜ãªæ–‡çŒ®')
            ãƒšãƒ¼ã‚¸ = metadata.get('ãƒšãƒ¼ã‚¸', 'ä¸æ˜ãªãƒšãƒ¼ã‚¸')
            ç«  = metadata.get('ç« ', '')
            ç¯€ = metadata.get('ç¯€', '')
            æ¨¹ç¨® = metadata.get('æ¨¹ç¨®', '')
            åŒºåˆ† = metadata.get('åŒºåˆ†', '')
            row_number = metadata.get('row_number', '')
            
            # ã‚­ãƒ¼ã®ä½œæˆï¼ˆæ–‡çŒ®åã¨ãƒšãƒ¼ã‚¸ã§è­˜åˆ¥ï¼‰
            key = f"{æ–‡çŒ®å}_page_{ãƒšãƒ¼ã‚¸}"
            
            if key not in doc_info:
                doc_info[key] = {
                    'æ–‡çŒ®å': æ–‡çŒ®å,
                    'ãƒšãƒ¼ã‚¸': ãƒšãƒ¼ã‚¸,
                    'ç« ': ç« ,
                    'ç¯€': ç¯€,
                    'æ¨¹ç¨®': set(),
                    'åŒºåˆ†': set(),
                    'contents': set()  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚setã‚’ä½¿ç”¨
                }
            
            # æ¨¹ç¨®ã¨åŒºåˆ†ã®æƒ…å ±ã‚’è¿½åŠ 
            if æ¨¹ç¨® and æ¨¹ç¨®.strip():
                doc_info[key]['æ¨¹ç¨®'].add(æ¨¹ç¨®)
            if åŒºåˆ† and åŒºåˆ†.strip():
                doc_info[key]['åŒºåˆ†'].add(åŒºåˆ†)
                
            # RAGã§å‚ç…§ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if show_content:
                doc_info[key]['contents'].add(doc.page_content)

        if doc_info:
            print("\n" + "=" * 60)
            print("ğŸ“š å‚ç…§ã—ãŸãƒ‡ãƒ¼ã‚¿:")
            print("=" * 60)
            
            # æ–‡çŒ®åã¨ãƒšãƒ¼ã‚¸ã§ã‚½ãƒ¼ãƒˆ
            sorted_docs = sorted(doc_info.values(), key=lambda x: (x['æ–‡çŒ®å'], str(x['ãƒšãƒ¼ã‚¸'])))
            
            for doc in sorted_docs:
                # åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
                info_parts = [f"ğŸ“„ {doc['æ–‡çŒ®å']}"]
                if doc['ãƒšãƒ¼ã‚¸']:
                    info_parts.append(f"ãƒšãƒ¼ã‚¸ {doc['ãƒšãƒ¼ã‚¸']}")
                
                print(f"\n{' '.join(info_parts)}")
                
                # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                details = []
                if doc['ç« ']:
                    details.append(f"ç« : {doc['ç« ']}")
                if doc['ç¯€']:
                    details.append(f"ç¯€: {doc['ç¯€']}")
                if doc['æ¨¹ç¨®']:
                    æ¨¹ç¨®_str = ", ".join(sorted(doc['æ¨¹ç¨®']))
                    details.append(f"æ¨¹ç¨®: {æ¨¹ç¨®_str}")
                if doc['åŒºåˆ†']:
                    åŒºåˆ†_str = ", ".join(sorted(doc['åŒºåˆ†']))
                    details.append(f"åŒºåˆ†: {åŒºåˆ†_str}")
                
                if details:
                    print(f"  {' | '.join(details)}")
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
                if show_content and doc['contents']:
                    print("  ğŸ“ å‚ç…§å†…å®¹:")
                    for idx, content in enumerate(sorted(doc['contents'])):  # é †åºã‚’ä¸€å®šã«
                        print(f"    [{idx+1}] {content[:300]}{'...' if len(content)>300 else ''}")
                        if idx < len(doc['contents']) - 1:
                            print("    " + "-" * 50)
            
            print("=" * 60)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='ç›†æ ½å°‚ç”¨RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ')
    parser.add_argument('--show-content', action='store_true', 
                       help='å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚‚è¡¨ç¤ºã™ã‚‹')
    parser.add_argument('--track-tokens', action='store_true', default=True, 
                       help='ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰')
    parser.add_argument('--no-track-tokens', action='store_true', 
                       help='ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¿½è·¡ã‚’ç„¡åŠ¹ã«ã™ã‚‹')
    args = parser.parse_args()
    
    # ãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ã®è¨­å®š
    track_tokens = args.track_tokens and not args.no_track_tokens
    
    try:
        chatbot = RAGChatBotShinpaku(show_content=args.show_content, track_tokens=track_tokens)
        chatbot.main()
    except Exception as e:
        print(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print("æ•´å‚™æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("å…ˆã« create_vectorstore_shinpaku.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
